---
title: "Bankruptcy Prediction"
author: "Giulio Bellini"
format: html
editor: visual
execute:
  warning: false
---

## Exploratory Data Analysis

```{r}
#| echo: false
# importing data from Excel
library(readxl)
#
library(dplyr)
#
library(caret)
#
library(DT)
#
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))
```

### Data Handling

A first step in the exploration analysis, we load the data from the workplace and convert it into a dataframe.

```{r}
rm(list=ls())  #clear environment

df <- as.data.frame(read_excel("data.xlsx"))
#View(df)
```

Looking at the dimensions of the dataset, we can see that it contains 6819 samples (companies) , 95 independent variables and 1 dependent variable. The features are all numerical and represent accounting values and ratios of the company to which they refer. The purpose of this research is to see whether the variables and the interactions between them can accurately predict the target variable, i.e. whether the company has gone bankrupt (Y=1) or not (Y=0).

The target variable is highly imbalanced, with only 220 bankruptcy cases, representing 3.2% of the total.

```{r}
#| echo: false
cat("The dimensions of the dataset is: ")
cat(dim(df), "\n")

cat("The distribution of the target variable is: ")
cat(table(df$`Bankrupt?`), "\n")

cat('The proportion of positiove cases is: ')
cat(round(nrow(df[df$`Bankrupt?`==1,])/nrow(df), 3))
```

The table below shows three main descriptive statistics (type, mean, standard deviation and number of missing values) for the dependent variable and each of the independent variables.

All variables are numeric. This is an advantage for modelling as there is no need to encode categorical variables. Furthermore, there are no missing values in the dataset, so there is no need to impute values in the data handling phase.

A closer look at the table shows that some variables have very small standard deviations. This means that the companies under consideration have very similar values for these variables or that the classes are highly imbalance.

Finally, the variable *"Net Income Flag"* has a standard deviation equal to 0, as it takes the value 1 for all samples (all companies have negative net income for the last two years). This variable does not help in classification and must be removed as it causes errors in the calculation of correlations, PCA, etc.

```{r}
# Calculate descriptive statistics

# Define a function to calculate mean for numeric columns only
mean_numeric <- function(x) {
  if(is.numeric(x)) mean(x, na.rm = TRUE) else NA
}

# Define a function to calculate standard deviation for numeric columns only
sd_numeric <- function(x) {
  if(is.numeric(x)) sd(x, na.rm = TRUE) else NA
}

# Calculate the statistics
stats <- data.frame(
  #Feature = names(df),
  DataType = sapply(df, class),
  Mean = sapply(df, mean_numeric),
  StdDev = sapply(df, sd_numeric),
  NumMissing = sapply(df, function(x) sum(is.na(x)))
)

# Convert to a tibble for nicer printing, if desired
datatable(stats, options = list(pageLength = 5))
```

```{r}
#Removing zero variance variable "Net Income Flag"
df <- subset(df, select= -`Net Income Flag`)
```

```{r}
#|echo: false
hist(df$`Inventory Turnover Rate (times)`)
```

```{r}
#| echo: false
"original_names <- colnames(df)
modified_names <- list()

for (index in seq(length(colnames(df)))) {
  if (index == 1) {
    modified_names <- c(modified_names, 'Y')
  }
  else {
    modified_names <- c(modified_names, paste0('X',as.character(index-1)))
  }
  
}

colnames(df) <- modified_names
"
```

A key step is to divide the data into two sets: the training set and the test set.

-   The ***training set*** will be used for correlation analysis, multicollinearity testing, dimensionality reduction, selecting the method for generating synthetic data, and training the classification models.

-   The ***test set*** is used to approximate the performance of the models on out-of-sample data.

It is important to separate the training and test sets before performing any data transformation, analysis or modelling to avoid data leakage between the test and training sets, which could lead to a positive bias (overestimation) in model performance.

The *X_train* and *X_test* sets are then scaled separately to avoid information leakage. Scaling the features so that they have a mean of 0 and a standard deviation of 1 is an important step because some algorithms are affected by the order of magnitude of the variables.

For example, in **Principal Component Analysis** (PCA), if a feature has a larger variance just because of its order of magnitude, it will have a larger effect on the direction of the principal component without any real motivation, leading to worse class separability in lower dimensions.

Another example is the **K-Nearest Neighbours** (KNN) algorithm, which is used both as a classification model and in other algorithms such as Isomap and SMOTE. In KNN, the use of unscaled features results in a completely different fit, as higher order variables have a greater impact on the distance between data points and thus on the selection of the k-nearest neighbours.

```{r}
# Create a stratified random sample to maintain proportion of target variable
set.seed(123) # Set seed for reproducibility
trainIndex <- createDataPartition(df$`Bankrupt?`, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

# Split the data into training and testing sets
# Split independent and dependent variables
# Scale sets separately to avoid data leakage
X_train <- as.data.frame(scale(select(df[trainIndex, ], -`Bankrupt?`)))
Y_train <- df[trainIndex, ]$`Bankrupt?`

X_test <- as.data.frame(scale(select(df[-trainIndex, ], -`Bankrupt?`)))
Y_test <- df[-trainIndex, ]$`Bankrupt?`

# Now, train set contains 80% of the data, and test set contains 20%, 
# with the proportion of the target variable's classes maintained.
```

### Correlation Analysis

The Table below show the **correlation** of each independent variable with the dependent variable "Bankrupt?".

In the graph on the right, the features are sorted in descending order according to their correlation coefficient with the independent variable.

It can be seen that variables related to the amount of liabilities relative to equity and assets are *positively* correlated with corporate bankruptcy. This makes sense from a financial point of view, since excessive use of debt, especially short-term debt, increases the risk of insolvency. Excessive expenses relative to asset value and excessive reliance on equity relative to long-term debt are also positively correlated with bankruptcy.

Variables related to profit, efficiency, such as ROA (return on assets) and share value are *negatively* correlated with company failure. Again, this makes financial sense. A company with high profitability is able to pay its debts on time, make investments for growth and face sudden expenses without running the risk of financial distress.

The correlation coefficients of the independent variables are mostly below 0.3 in absolute terms. Therefore, the features are only weakly positively correlated with the dependent variable. This is a common occurrence in economics and finance, as many factors affect the economic system. However, this does not necessarily mean that these variables and their interactions lack predictive power.

```{r}
# Perforing Pearson Correlation Test on the Training Set
# Defining empty dataframe to store the results: corr coefficients and p-values
test_results <- data.frame()

# iterate along the columns and compute correlation coefficent and p-value for each independent variable with the dependent variable
for (col in 1:(ncol(X_train) -1)) {
  result <- cor.test(Y_train, X_train[, col])
  result_df <- data.frame(index=colnames(X_train)[col], coeff=round(result$estimate,4), pvalue=round(result$p.value, 5))
  test_results <- rbind(test_results, result_df)
}
# Setting features names as row names
row.names(test_results) <- test_results$index
test_results$index <- NULL

# Creating interactive table to visualize the results
# Features are sorted in descending order based on the correlation coefficient
datatable(test_results[order(-test_results$coeff),], options = list(pageLength = 5))
```

Figure shows the correlation matrix of the independent variables. The names of the variables have been omitted to avoid cluttering the graph. The Figure shows the correlation matrix of the variables that have a correlation coefficient greater than or equal to 0.9 in absolute terms with any other independent variable.

In both correlation matrices, the variables were rearranged using hierarchical clustering to identify possible clusters of highly correlated variables. Hierarchical clustering was applied to the distance matrix obtained from the correlation matrix, where distances were calculated as $1 - |correlationCoeff|$. The features were then reordered so that variables belonging to the same cluster were positioned next to each other in the matrix. This procedure makes the correlation matrix easier to interpret.

In Figure we can see two main clusters: one of positively correlated features in the centre of the plot and one of negatively correlated features at the top. Other smaller clusters are also visible. Looking at Figure, these two clusters are even more obvious.

```{r}
library(corrplot)
#par(mfrow = c(1, 2))

cor_matrix <- cor(X_train)
Y_cor <-cor(X_train, Y_train, use = "complete.obs")
index_order <- order(Y_cor, decreasing=TRUE)
Y_cor_sorted <- Y_cor[index_order]
Y_cor_sorted_names <- rownames(Y_cor)[index_order]

Y_cor_matrix <- matrix(Y_cor_sorted, nrow = length(Y_cor_sorted), ncol = 1, dimnames = list(Y_cor_sorted_names, c("Bankrupt?")))

# Adjust plot size
#options(repr.plot.width=14, repr.plot.height=30)

# Plot correlation matrix
#par(mar=c(2,2,2,2))  # Adjust margin to fit long variable names

col <- colorRampPalette(c("blue", "white", "red"))(200)

#corrplot(Y_cor_matrix, is.corr = FALSE, method = "color",
       #tl.col = "black", col=col, tl.cex = 0.6, tl.srt=0, cl.pos="n")

corrplot(cor_matrix, method = "color", type = "upper",
         tl.pos = "n",
         diag = FALSE, # Exclude diagonal
         order = "hclust", # Order variables by hierarchical clustering
         tl.col = "black", # Set text label color
         #tl.cex = 0,
         col=col)# Adjust text label size 

#title("Correlation Matrix")
```

Some features are perfectly correlated:

$Corr(Current\ Liability\ to\ Equity,\ Current\ Liabilities/Equity) = 1$

$Corr(Current\ Liabilities\ to\ Liability,\ Current\ Liabilities/Liability) = 1$

$Corr(Debt\ Ratio, Net\ Worth/Assets) = -1$

These variables seem to measure the same quantity, even if their names are slightly different. For each pair of perfectly correlated variables, it is necessary to select only one, as the other would complicate the models without providing any useful information.

Other variables are highly correlated, even if not perfectly. These variables express very similar accounting aspects, differing only in small details. Some examples are:

$Corr(Operating\ Gross\ Margin,\ Gross\ Profit\ to\ Sales) = 0.999$

$Corr(Pre-tax\ net\ Interest\ Rate,\ After-tax\ net\ Interest\ Rate) = 0.987$

$Corr(ROA(A)\ before\ interest,\ Net\ Income\ to\ Total\ Assets) = 0.96$

Unlike in the case of perfectly correlated variables, in this case it is not the best choice to choose one random variable to keep and disregard the other, because we may miss important information, such as interactions with other variables. It is therefore necessary to proceed with further analysis.

```{r, fig.width=15, fig.height=15}
library(corrplot)

# Identify features with correlation > 0.9 or < -0.9 with any other feature
# Exclude diagonal elements by setting them to 0 as we don't consider self-correlation
diag(cor_matrix) <- 0
high_cor_features <- which(apply(cor_matrix, 1, function(x) any(x > 0.9 | x < -0.9)), arr.ind = TRUE)

# Subset the correlation matrix
sub_cor_matrix <- cor_matrix[high_cor_features, high_cor_features]

# Define a custom color sequence from blue to red
col <- colorRampPalette(c("blue", "white", "red"))(200)

# Plot the subsetted correlation matrix
#par(mar = c(0,0, 0, 0))

#options(repr.plot.width=2, repr.plot.height=3)

corrplot(sub_cor_matrix, method = "color", type="upper",
         order = "hclust", # Cluster variables
         addrect = 3, # Adjust the number of rectangles for clustering as needed
         diag= FALSE,
         tl.col = "black", tl.srt = 45, tl.cex = 0.75, # Adjust text appearance
         col = col) # Use the custom color sequence

# Add a title
#title("Subset Correlation Matrix")
```

## Dimensionality Reduction

```{r}
library(ggplot2)
# Perform PCA
pca_result <- prcomp(X_train) #, scale. = TRUE

# Extract the first two principal components
pc1 <- pca_result$x[, 1]
pc2 <- pca_result$x[, 2]

# Create a dataframe for plotting
pca_df <- data.frame(PC1 = pc1, PC2 = pc2, Y=factor(Y_train, labels = c("Non Bankrupt", "Bankrupt")))

# Plot the first two principal components
ggplot(pca_df, aes(x = PC1, y = PC2, color=Y)) +
  geom_point(alpha=0.7) +
  labs(x = "Principal Component 1", y = "Principal Component 2", color='Y',title = "PCA: First Two Principal Components")
```

```{r}
# warning: false
#| eval: false
library(Rtsne)

# Perform t-SNE
tsne_result <- Rtsne(X_train)

tsne_df <- data.frame(tsne_result$Y, Y= factor(Y_train, labels = c("Non Bankrupt", "Bankrupt")))

# Plot t-SNE result
ggplot(tsne_df, aes(x = X1, y = X2, color = Y)) +
  geom_point(alpha=0.7) +
  labs(title = "t-SNE Plot")
```

```{r}
#| eval: false
#Exporting train and test sets
#library(openxlsx)

#write.xlsx(trainSet, file = "train.xlsx", sheetName = "train", rowNames = FALSE)

#write.xlsx(testSet, file = "test.xlsx", sheetName = "test", rowNames = FALSE)
```
