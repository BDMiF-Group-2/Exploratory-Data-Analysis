---
title: "Bankruptcy Prediction"
author: "Giulio Bellini"
format: html
editor: visual
execute:
  warning: false
---

## Exploratory Data Analysis

```{r}
#| echo: false
# importing data from Excel
library(readxl)
# 
library(dplyr)
#
library(caret)
#
#library(DT)
#
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))
```

### Data Handling

A first step in the exploration analysis, we load the data from the workplace and convert it into a dataframe.

```{r}
# Clear environment
rm(list=ls())

# Import Data
df <- as.data.frame(read_excel("data.xlsx"))
```

Looking at the dimensions of the dataset, we can see that it contains 6819 samples (companies) , 95 independent variables and 1 dependent variable. The features are all numerical and represent accounting values and ratios of the company to which they refer. The purpose of this research is to see whether the variables and the interactions between them can accurately predict the target variable, i.e. whether the company has gone bankrupt (Y=1) or not (Y=0).

The target variable is highly imbalanced, with only 220 bankruptcy cases, representing 3.2% of the total.

```{r}
#| echo: false
cat("The dimensions of the dataset is: ")
cat(dim(df), "\n")

cat("The distribution of the target variable is: ")
cat(table(df$`Bankrupt?`), "\n")

cat('The proportion of positiove cases is: ')
cat(round(nrow(df[df$`Bankrupt?`==1,])/nrow(df), 3))
```

The table below shows three main descriptive statistics (type, mean, standard deviation and number of missing values) for the dependent variable and each of the independent variables.

All variables are numeric. This is an advantage for modelling as there is no need to encode categorical variables. Furthermore, there are no missing values in the dataset, so there is no need to impute values in the data handling phase.

```{r}
# Calculate descriptive statistics

# Define a function to calculate mean for numeric columns only
median_numeric <- function(x) {
  if(is.numeric(x)) median(x, na.rm = TRUE) else NA
}

# Define a function to calculate standard deviation for numeric columns only
sd_numeric <- function(x) {
  if(is.numeric(x)) sd(x, na.rm = TRUE) else NA
}

# Calculate the statistics and store them in a data frame
stats <- data.frame(
  Feature = colnames(df),
  DataType = sapply(df, class),
  Median = sapply(df, median_numeric),
  StdDev = sapply(df, sd_numeric),
  NumMissing = sapply(df, function(x) sum(is.na(x)))
)

# Display interactive table
#datatable(stats, options = list(pageLength = 5))
```

```{r}
library(insight)
export_table(format_table(stats), format='html')
```

```{r}
#| eval: false
#| echo: false
k <- log(max(df[,72]))/log(exp(1)-1)
feature <- log(df[,72]^(1/k)+1)

boxplot(feature, horizontal=TRUE)
```

A closer analysis shows that while some variables have very small standard deviations, others have standard deviations in the billions (e+09). The latter variables have extremely right-skewed distributions, with values ranging from zero to several billions. The box-plots of three sample variables are shown in Figure. The extreme values could be caused by human error in data collection, but the data source (Taiwan Economic Journal) is sufficiently reliable to rule out this possibility. Therefore, assuming that the data are correct, we could apply transformations (e.g. logarithmic) to the variables to shrink the extreme values towards the median. However, we decided not to apply any transformations beyond scaling the data to preserve the shape of the feature distributions, as outliers could be useful in identifying bankrupt companies.

```{r}
# Show boxplots of three features

# Show the three plots arranged in a column
par(mfrow = c(3, 1))

boxplot(df$'Quick Asset Turnover Rate',
        main= "Sample Features Boxplots",
        xlab="Quick Asset Turnover Rate",
        col="darkmagenta",
        horizontal=TRUE)


boxplot(df$'Revenue per person',
        xlab = "Revenue per person",
        col="blue",
        horizontal=TRUE)

boxplot(df$'Long-term Liability to Current Assets',
     xlab="Long-term Liability to Current Assets",
     col="green",
     horizontal=TRUE)
```

Finally, the variable *"Net Income Flag"* has a standard deviation equal to 0, as it takes the value 1 for all samples (all companies have negative net income for the last two years). This variable does not help in classification and must be removed as it causes errors in the calculation of correlations, PCA, etc.

```{r}
#Removing zero variance variable "Net Income Flag"
df <- subset(df, select= -`Net Income Flag`)
```

```{r}
#| echo: false
#| eval: false
"original_names <- colnames(df)
modified_names <- list()

for (index in seq(length(colnames(df)))) {
  if (index == 1) {
    modified_names <- c(modified_names, 'Y')
  }
  else {
    modified_names <- c(modified_names, paste0('X',as.character(index-1)))
  }
  
}

colnames(df) <- modified_names"

```

A key step is to divide the data into two sets: the training set and the test set.

-   The ***training set*** will be used for correlation analysis, multicollinearity testing, dimensionality reduction, selecting the method for generating synthetic data, and training the classification models.

-   The ***test set*** is used to approximate the performance of the models on out-of-sample data.

It is important to separate the training and test sets before performing any data transformation, analysis or modelling to avoid data leakage between the test and training sets, which could lead to a positive bias (overestimation) in model performance.

The *X_train* and *X_test* sets are then scaled separately to avoid information leakage. Scaling the features so that they have a mean of 0 and a standard deviation of 1 is an important step because some algorithms are affected by the order of magnitude of the variables.

For example, in **Principal Component Analysis** (PCA), if a feature has a larger variance just because of its order of magnitude, it will have a larger effect on the direction of the principal component without any real motivation, leading to worse class separability in lower dimensions.

Another example is the **K-Nearest Neighbours** (KNN) algorithm, which is used both as a classification model and in other algorithms such as Isomap and SMOTE. In KNN, the use of unscaled features results in a completely different fit, as higher order variables have a greater impact on the distance between data points and thus on the selection of the k-nearest neighbours.

```{r}
# Create a stratified random sample to maintain proportion of target variable
set.seed(123) # Set seed for reproducibility
trainIndex <- createDataPartition(df$`Bankrupt?`, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

# Split the data into training and testing sets
# Split independent and dependent variables
# Scale sets separately to avoid data leakage
X_train <- as.data.frame(scale(select(df[trainIndex, ], -`Bankrupt?`)))
Y_train <- df[trainIndex, ]$`Bankrupt?`

X_test <- as.data.frame(scale(select(df[-trainIndex, ], -`Bankrupt?`)))
Y_test <- df[-trainIndex, ]$`Bankrupt?`

# Now, train set contains 80% of the data, and test set contains 20%, 
# with the proportion of the target variable's classes maintained.
```

### Correlation Analysis

The Table below show the **correlation** of each independent variable with the dependent variable "Bankrupt?".

In the graph on the right, the features are sorted in descending order according to their correlation coefficient with the independent variable.

It can be seen that variables related to the amount of liabilities relative to equity and assets are *positively* correlated with corporate bankruptcy. This makes sense from a financial point of view, since excessive use of debt, especially short-term debt, increases the risk of insolvency. Excessive expenses relative to asset value and excessive reliance on equity relative to long-term debt are also positively correlated with bankruptcy.

Variables related to profit, efficiency, such as ROA (return on assets) and share value are *negatively* correlated with company failure. Again, this makes financial sense. A company with high profitability is able to pay its debts on time, make investments for growth and face sudden expenses without running the risk of financial distress.

The correlation coefficients of the independent variables are mostly below 0.3 in absolute terms. Therefore, the features are only weakly positively correlated with the dependent variable. This is a common occurrence in economics and finance, as many factors affect the economic system. However, this does not necessarily mean that these variables and their interactions lack predictive power.

```{r}
# Perform Pearson Correlation Test on the Training Set
# Define empty dataframe to store the results: corr coefficients and p-values
test_results <- data.frame()

# Iterate along the columns and compute correlation coefficent and p-value
# for each independent variable with the dependent variable
for (col in 1:(ncol(X_train) -1)) {
  result <- cor.test(Y_train, X_train[, col])
  result_df <- data.frame(index=colnames(X_train)[col], coeff=round(result$estimate,4), pvalue=round(result$p.value, 5))
  test_results <- rbind(test_results, result_df)
}
# Set features names as row names
row.names(test_results) <- test_results$index
test_results$index <- NULL

# Create interactive table to visualize the results
# Features are sorted in descending order based on the correlation coefficient
#datatable(test_results[order(-test_results$coeff),], options = list(pageLength = 5))
test_results <- test_results[order(-test_results$coeff),]
test_results <- cbind(rownames(test_results), test_results)
colnames(test_results)[1] <- "Feature"
export_table(format_table(test_results), format='html')
```

Figure shows the correlation matrix of the independent variables. The names of the variables have been omitted to avoid cluttering the graph. The Figure shows the correlation matrix of the variables that have a correlation coefficient greater than or equal to 0.9 in absolute terms with any other independent variable.

In both correlation matrices, the variables were rearranged using hierarchical clustering to identify possible clusters of highly correlated variables. Hierarchical clustering was applied to the distance matrix obtained from the correlation matrix, where distances were calculated as $1 - |correlationCoeff|$. The features were then reordered so that variables belonging to the same cluster were positioned next to each other in the matrix. This procedure makes the correlation matrix easier to interpret.

In Figure we can see two main clusters: one of positively correlated features in the centre of the plot and one of negatively correlated features at the top. Other smaller clusters are also visible. Looking at Figure, these two clusters are even more obvious.

```{r}
#| echo: false
#| eval: false

Y_cor <-cor(X_train, Y_train, use = "complete.obs")
index_order <- order(Y_cor, decreasing=TRUE)
Y_cor_sorted <- Y_cor[index_order]
Y_cor_sorted_names <- rownames(Y_cor)[index_order]

Y_cor_matrix <- matrix(Y_cor_sorted, nrow = length(Y_cor_sorted), ncol = 1, dimnames = list(Y_cor_sorted_names, c("Bankrupt?")))

# Adjust plot size
#options(repr.plot.width=14, repr.plot.height=30)

# Plot correlation matrix
#par(mar=c(2,2,2,2))  # Adjust margin to fit long variable names

#corrplot(Y_cor_matrix, is.corr = FALSE, method = "color",
       #tl.col = "black", col=col, tl.cex = 0.6, tl.srt=0, cl.pos="n")
```

```{r}
# Import library to show correlation plot
library(corrplot)

# Compute correlation matrix of the indenpendent variables
# Correlation analysis is performed on train set only 
cor_matrix <- cor(X_train)

# Define custom color sequence from blue to red
# Blue for negative correlation and red for positive correlation
col <- colorRampPalette(c("blue", "white", "red"))(200)

# Show correlation matrix
corrplot(cor_matrix, method = "color", type = "upper",
         tl.pos = "n", # Don't show features names
         diag = FALSE, # Exclude diagonal
         order = "hclust", # Order variables by hierarchical clustering
         col=col) # Set custom color sequence 
```

Some features are perfectly correlated:

$Corr(Current\ Liability\ to\ Equity,\ Current\ Liabilities/Equity) = 1$

$Corr(Current\ Liability\ to\ Liability,\ Current\ Liabilities/Liability) = 1$

$Corr(Debt\ Ratio, Net\ worth/Assets) = -1$

These variables seem to measure the same quantity, even if their names are slightly different. For each pair of perfectly correlated variables, it is necessary to select only one, as the other would complicate the models without providing any useful information.

Other variables are highly correlated, even if not perfectly. These variables express very similar accounting aspects, differing only in small details. Some examples are:

$Corr(Operating\ Gross\ Margin,\ Gross\ Profit\ to\ Sales) = 0.999$

$Corr(Pre-tax\ net\ Interest\ Rate,\ After-tax\ net\ Interest\ Rate) = 0.987$

$Corr(ROA(A)\ before\ interest,\ Net\ Income\ to\ Total\ Assets) = 0.96$

Unlike in the case of perfectly correlated variables, in this case it is not the best choice to choose one random variable to keep and disregard the other, because we may miss important information, such as interactions with other variables. It is therefore necessary to proceed with further analysis.

```{r, fig.width=15, fig.height=15}
library(corrplot)

# Identify features with correlation > 0.9 or < -0.9 with any other feature
# Exclude diagonal elements by setting them to 0 as we don't consider self-correlation
diag(cor_matrix) <- 0
high_cor_features <- which(apply(cor_matrix, 1, function(x) any(x > 0.9 | x < -0.9)), arr.ind = TRUE)

# Subset the correlation matrix
sub_cor_matrix <- cor_matrix[high_cor_features, high_cor_features]

# Define custom color sequence from blue to red
col <- colorRampPalette(c("blue", "white", "red"))(200)

# Plot the subsetted correlation matrix

corrplot(sub_cor_matrix, method = "color", type="upper",
         order = "hclust", # Cluster variables
         addrect = 3, # Adjust the number of rectangles for clustering as needed
         diag= FALSE, # Exclude diagonal
         tl.col = "black", tl.srt = 45, tl.cex = 0.75, # Adjust text appearance
         col = col) # Set custom color sequence
```

```{r}
# Remove on of two perfectly correlated features
# Remove both in train and test set
X_train <- subset(X_train, select= -c(`Current Liability to Equity`,`Current Liability to Liability`,`Net worth/Assets`))

X_test <- subset(X_test, select= -c(`Current Liability to Equity`,`Current Liability to Liability`,`Net worth/Assets`))
```

## Dimensionality Reduction

Dimensionality reduction by **Principal Components Analysis** (PCA) was performed on the training set and the Figure shows the first two principal components. We can see a large central cluster and several outliers. Some of the outliers correspond to bankrupt companies. We expect these data points to be easily classified correctly by econometric or machine learning models. However, most bankrupt firms are mixed with non-bankrupt firms in the central cluster. This means that these positive cases are close to the negative cases in the high dimensional space and are more difficult to classify correctly. However, it can be seen that almost all bankrupt companies have a negative value for the first principal component, suggesting that some separability of these cases is still possible.

```{r}
# Perform PCA
pca_result <- prcomp(X_train)

# Extract the first two principal components
pc1 <- pca_result$x[, 1]
pc2 <- pca_result$x[, 2]

# Create a dataframe for plotting
pca_df <- data.frame(PC1 = pc1, PC2 = pc2, Y=factor(Y_train, labels = c("Non Bankrupt", "Bankrupt")))

# Plot the first two principal components
# Datapoints corresponding to bankrupt companies are in the foregound 
ggplot(pca_df, aes(x = PC1, y = PC2)) +
  geom_point(data = pca_df[pca_df$Y == "Non Bankrupt", ], aes(color=Y)) +
  geom_point(data = pca_df[pca_df$Y == "Bankrupt", ], aes(color=Y)) +
  labs(x = "PC1", y = "PC2",col="Bankrupt?",title = "PCA: First Two Principal Components") +
  theme(plot.title = element_text(hjust = 0.5))+
  theme_minimal()
```

```{r}
#| warning: false
#| eval: false
#| echo: false
library(Rtsne)

# Perform t-SNE
tsne_result <- Rtsne(X_train)

tsne_df <- data.frame(tsne_result$Y, Y= factor(Y_train, labels = c("Non Bankrupt", "Bankrupt")))

# Plot t-SNE result
ggplot(tsne_df, aes(x = X1, y = X2, color = Y)) +
  geom_point(alpha=0.7) +
  labs(title = "t-SNE Plot")
```

```{r}
#| eval: false
#| echo: false
#Exporting train and test sets
#library(openxlsx)

#write.xlsx(trainSet, file = "train.xlsx", sheetName = "train", rowNames = FALSE)

#write.xlsx(testSet, file = "test.xlsx", sheetName = "test", rowNames = FALSE)
```
